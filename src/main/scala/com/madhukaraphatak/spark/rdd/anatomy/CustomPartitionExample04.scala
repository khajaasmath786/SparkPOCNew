package com.madhukaraphatak.spark.rdd.anatomy

import org.apache.spark.{Partitioner, SparkContext}
import org.apache.spark.SparkContext._
import com.mcd.sparksql.util.DaasUtil

/**
 * Created by madhu on 11/3/15.
 */
object CustomPartitionExample03{


  
  def main(args: Array[String]) {
    
    if (args.length < 0) {
      println("Please specify Input Path")
      System.exit(-1)
    }
    val mapProps = DaasUtil.getConfig("Daas.properties")
    val master=DaasUtil.getValue(mapProps, "Master")
    val driverMemory=DaasUtil.getValue(mapProps, "Driver.Memory")
    val executorMemory=DaasUtil.getValue(mapProps, "Executor.Memory")
    val jobName="PartitionExample"
    val conf = DaasUtil.getJobConf(jobName, master, executorMemory, driverMemory);
    val sc = new SparkContext(conf)

    val salesData = sc.textFile(args(0)) // src/main/resources/sales.csv

    val salesByCustomer = salesData.map(value => {
      val colValues = value.split(",")
      (colValues(1),colValues(3).toDouble)
    })

    
    /* $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
     * 
     * Partition for transformed Data Slides..
     * with custom partitoner, keys are distributed evenly across partiioners
     *  --> for transformed data inside the code(like you get rdd from other rdd), number of partitions is not transformed by the size but driven by logic called partitoner. 
     * Partitioning will be different for key/value pairs that are generated by shuffle operation. (groupby, reduce by ect which has key value pairs. Partitoners will be different)
     * Partitioning is driven by partitioner specified by default HashPartitioner is used You can use your own partitioner also
     * Partitioner= key%numberofPartitons   -->(numofPartitoons by developer but partitoner programatically when we transform rdd with key values pairs)
     * 
     * ---> if you see slide, we started with 3 partitions in figure but ended up with two partitions for pair keys.Partitioner= key%numberofPartitons. 
     * ---->indirectly called bucketing. simarlar to indexing in rdbms like team connect project
     * ----> grouping needs shuffling .. imp point .. when shuffling is needed, partitons will change.
     * ----> this is very similar to number of reducers we want ..
     * 
     * --> to distribute uniformly, we use custom partitioner so that some keys wont get more data whiel othere dont take more time if they have less data for a key.
     * */
    
    
    val groupedData = salesByCustomer.groupByKey(new CustomerPartitioner) // groupByKey will take number of partitions also instead of custom partitioner
    // group by key, reduce by key will 
    println("Number of partitions  "+groupedData.partitions.length)
    //printing partition specific data

    val groupedDataWithPartitionData = groupedData.mapPartitionsWithIndex{
      case(partitionNo,iterator) => {
       List((partitionNo,iterator.toList.length)).iterator
      }
    }

    println(groupedDataWithPartitionData.collect().toList)
    
    
    
  }
  

}
